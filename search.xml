<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SPP-net]]></title>
    <url>%2F2018%2F04%2F14%2FSPP-net%2F</url>
    <content type="text"><![CDATA[为了解决现有CNN需要固定输入大小的问题，提出了SPP-net，使得针对任意尺寸的图像生成固定长度的特征表示。SPP-net不仅可以应用在分类任务上，而且在检测任务上也有很大的性能提升。 IntroductionCNN正在取得快速的发展，然而在CNN的训练和测试阶段都有一个技术问题：CNN需要固定输入图像的尺寸，这些图片或者经过裁切（crop）或者经过变形缩放（warp），都在一定程度上导致图片信息的丢失和变形，限制了识别精确度。 如下图所示，上面是CNN一般的做法，对不符合网络输入大小的图像直接进行crop或warp，下面是SPP-net的工作方式。SPP-net加在最后一个卷积层的输出后面，使得不同输入尺寸的图像在进过前面的卷积池化过程后，再经过SPP-net，得到相同大小的feature map，最后再经过全连接层进行分类。 Spatital Pyramid Pooling卷积层是不需要输入固定大小的图片的，而且还可以生成任意大小的特征图，只是全连接层需要固定大小的输入。因此，固定长度的约束仅限于全连接层。在本文中提出了Spatial Pyramid Pooling layer 来解决这一问题，使用这种方式，可以让网络输入任意的图片，而且还会生成固定大小的输出。 以下图为例，黑色图片代表卷积之后的特征图，接着我们以不同大小的块来提取特征，分别是4*4，2*2，1*1，将这三张网格放到下面这张特征图上，就可以得到16+4+1=21种不同的块(Spatial bins)，我们从这21个块中，每个块提取出一个特征，这样刚好就是我们要提取的21维特征向量。这种以不同的大小格子的组合方式来池化的过程就是空间金字塔池化（SPP）。比如，要进行空间金字塔最大池化，其实就是从这21个图片块中，分别计算每个块的最大值，从而得到一个输出单元，最终得到一个21维特征的输出。 金字塔池化的意义总结而言，当网络输入的是一张任意大小的图片，这个时候我们可以一直进行卷积、池化，直到网络的倒数几层的时候，也就是我们即将与全连接层连接的时候，就要使用金字塔池化，使得任意大小的特征图都能够转换成固定大小的特征向量，这就是空间金字塔池化的意义（多尺度特征提取出固定大小的特征向量）。 SPP-net在目标检测中的应用对于R-CNN，整个过程是： 首先通过选择性搜索，对待检测的图片进行搜索出~2000个候选窗口。 把这2k个候选窗口的图片都缩放到227*227，然后分别输入CNN中，每个proposal提取出一个特征向量，也就是说利用CNN对每个proposal进行提取特征向量。 把上面每个候选窗口的对应特征向量，利用SVM算法进行分类识别。 可以看出R-CNN的计算量是非常大的，因为2k个候选窗口都要输入到CNN中，分别进行特征提取。 而对于SPP-Net，整个过程是： 首先通过选择性搜索，对待检测的图片进行搜索出2000个候选窗口。这一步和R-CNN一样。 特征提取阶段。这一步就是和R-CNN最大的区别了，这一步骤的具体操作如下：把整张待检测的图片，输入CNN中，进行一次性特征提取，得到feature maps，然后在feature maps中找到各个候选框的区域，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量。而R-CNN输入的是每个候选框，然后在进入CNN，因为SPP-Net只需要一次对整张图片进行特征提取，速度会大大提升。 最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别。 下图为SPP-net进行目标检测的完整步骤： Reference Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (PAMI, 2015) SPPnet论文总结 SPP-Net论文详解 Caffe源码实现]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-CNN]]></title>
    <url>%2F2018%2F04%2F14%2FR-CNN%2F</url>
    <content type="text"><![CDATA[R-CNN，结合Region Proposal和CNN的目标检测算法。使用Region Proposal (Selective Search)算法得到有可能是目标的若干图像局部区域，然后把这些区域分别输入到CNN中，得到区域的特征，加上SVM分类器，判断特征对应的区域是属于某类目标还是背景，最后进行边框回归。 Introduction在R-CNN之前，目标检测已经到了瓶颈期，一些最好的方法都是结合底层图像特征和高层语义的复杂集成系统。而这篇论文在VOC2012的mAP达到了53.3%，比之前最好的方法高30%。该方法主要有两个关键点：1）一个是将CNN应用在了自底向上的区域候选框上，用来定位和分割目标；2）当标记的训练数据比较少时，采用有监督预训练，并进行微调，可以显著提高性能。 R-CNNOverviewR-CNN的目标检测系统由三部分组成：第一步生成region proposal，采用selective search算法；第二步使用CNN提取固定长度的特征向量；第三步使用线性SVM分类。 Training 有监督预训练：在ILSVRC2012分类数据集上预训练。 微调：使用目标检测的数据进行微调，以0.001的学习速率进行SGD训练。对某个分类只要IoU&gt;0.5就视该边框为正值。每次SGD迭代都采样38个正边框和96个背景。 分类器：对某个分类，高IoU和低IoU都很好区分，但IoU处于中值时则很难定义生成的候选框是否包含了该物体。作者设定了一个阈值0.3（从一系列阈值中选择的，0,0.1,…,0.5），低于它的一律视为背景（负数）。另外，每个分类都优化一个SVM。由于负样本很多，因此还采用了hard negative mining方法。 Testing 采用selective search提取约2000个region proposal，将每个region proposal缩放成227*227，利用CNN前向运算计算特征。然后，使用训练好的SVM对特征进行分类，最后采用NMS去除多余的region。 运行时间分析：有两个因素使得检测效率较高，首先所有CNN参数对所有类别都是共享的，其次，由CNN计算出来的特征向量相比其他方法是低维特征。 一些问题 可以不进行特定样本下的微调吗？可以直接采用AlexNet网络的特征进行SVM训练吗? ⽂中设计了没有进⾏微调的对⽐实验，分别就AlexNet⽹络的pool5、fc6、fc7层进⾏特征提取，输⼊SVM进⾏训练，这相当于把AlexNet网络作为特征提取器，类似HOG、SIFT等做特征提取⼀样，不针对特征任务。实验结果发现f6层提取的特征⽐f7层的mAP还⾼，pool5层提取的特征与f6、f7层相⽐mAP差不多； 在PASCAL VOC 2007数据集上采取了微调后fc6、fc7层特征较pool5层特征⽤于SVM训练提升mAP⼗分明显； 由此作者得出结论：不针对特定任务进⾏微调，⽽将CNN当成特征提取器，pool5层得到的特征是基础特征，类似于HOG、SIFT，类似于只学习到了⼈脸共性特征；从fc6和fc7等全连接层中所学习到的特征是针对特征任务特定样本的特征，类似于学习到了分类性别分类年龄的个性特征。 为什么微调时和训练SVM时所采⽤的正负样本阈值【0.5和0.3】不⼀致？ 微调阶段是由于CNN对⼩样本容易过拟合，需要⼤量训练数据，故对IoU限制宽松：与Ground Truth相交IoU&gt;0.5的建议框为正样本，否则为负样本； SVM这种机制是由于其适⽤于⼩样本训练，故对样本IoU限制严格：Ground Truth为正样本，与Ground Truth相交IoU＜0.3的建议框为负样本。 为什么不直接采⽤微调后的AlexNet⽹络最后⼀层SoftMax进⾏21分类【20类+背景】？ 因为微调时和训练SVM时所采⽤的正负样本阈值不同，微调阶段正样本定义并不强调精准的位置，⽽SVM正样本只有Ground Truth；并且微调阶段的负样本是随机抽样的，⽽SVM的负样本是经过hard negative mining⽅法筛选的；导致在采⽤SoftMax会使PSACAL VOC 2007测试集上mAP从54.2%降低到50.9%。 R-CNN速度慢在哪里？ RCNN存在着重复计算的问题（proposal的region有几千个，多数都是互相重叠，重叠部分会被多次重复提取feature）。 Reference Rich feature hierarchies for accurate object detection and semantic segmentation Caffe源码实现 R-CNN论文详解]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spatial Transformer Networks]]></title>
    <url>%2F2018%2F04%2F12%2FSpatial-Transformer-Networks%2F</url>
    <content type="text"><![CDATA[传统CNN通过扩增数据获得数据的一些不变性，如旋转不变性，平移不变性等，是一种隐式的学习，而空间变换网络（Spatial Transformer Networks，简称STN）通过显式学习数据的各种变换参数（如旋转，平移，仿射变换等）来获得这些变换的不变性。这个网络可以很方便的插入到已有的CNN网络中。 Introduction 不变性（invariance）：在CNN中，pooling层使得网络对位置的敏感度越来越低，从而使网络具有一定的平移不变性。这种空间不变性是通过多层的conv和pooling层实现的，而且对变换较大的输入数据并不具有不变性。 Spatial Transformer: 引入空间变换模块（spatial transformer module），可以添加到任何一个标准的神经网络结构中，从而提供空间变换的能力。与pooling层不同，空间变换模块是一个动态的机制，训练得到合适的变换参数，然后将这个变换应用在整个feature map上，可以覆盖scaling, cropping, ratations等变换。这样的好处是可以使得网络选择出一幅图像中最相关的区域，如下图所示，(a)是输入图像，(b)是空间变换模块预测的定位结果，(c)是空间变换后的结果，(d)是分类结果。 应用：STN可以嵌入CNN中实现不同的任务，例如： image classification: STN可以crop out和scale-normalize合适的区域，简化接下来的分类任务，提高分类性能； co-localisation: 不需要标记目标的位置，通过STN，可以定位目标； spatial attention: 可以实现attention机制，很灵活，无需增强学习。 Spatial Transformers由以下三部分组成 Localisation Network: 输入feature map，输出$\theta$。$\theta$的尺寸取决于变换类型，比如对于仿射变换，$\theta$就是6维的。这个网络可以采用任意形式，比如全连接网络或卷积网络，但是最后应该有一个回归层输出参数$\theta$。 Parameterised Sampling Grid: 以下是将Parameterised Sampling Grid应用到图像U上得到输出V的两个例子，图(a)是一个单位变换，图(b)是一个仿射变换。 Differentiable Image Sampling: 这一步是将输入的feature map结合变换参数，输出结果feature map。 个人总结传统CNN通过各种数据扩增操作来获得某些不变性（如平移不变性，旋转不变性等），是一种“隐式学习”的方法，即我们只是向网络输入数据，直接输出的就是预测结果。而STN主要是一种“显式学习”空间变换参数的方法，它通过训练得到这些参数，对输入数据（也可以是中间的feature map）进行变换，使得后续的处理能得到更好的结果。 优点：容易扩展，可以很方便的嵌入其它网络中；与CNN是直接识别出扭曲图像不同，STN显然更符合人类识别的过程，即先将图像纠正再识别。 缺点：训练时依然需要大量数据扩增才能学到所需要的不变性，而且学习到的变换参数不一定是我们希望的。 复现过程：对Mnist数据集进行随机旋转，然后使用论文中提到的超参数设置，结果显示采用STN后在测试集上的准确率确实高于不使用STN的时候。但是对STN层后的输出进行可视化后并没有原文中效果那么好，比如我需要学习的是旋转不变性，但是结果显示STN层所做的事情是平移而不是旋转。我认为可能在学习旋转不变性时需要固定6个变换参数中的某几个，使得能够显式的学习旋转不变性。 Reference Spatial Transformer Networks (NIPS, 2015) 李宏毅-深度学习-Spatial Transformer Layer Paper Reading：Spatial Transformer Networks（with code explanation） Caffe实现版本 Tensorflow实现版本 PyTorch实现版本 MXNet实现版本]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
</search>
