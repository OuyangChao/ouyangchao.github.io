<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[R-FCN]]></title>
    <url>%2F2018%2F04%2F23%2FR-FCN%2F</url>
    <content type="text"><![CDATA[R-FCN是在Faster R-CNN的框架上进行改造，第一，把base network由VGG16换成了ResNet，第二，把Fast R-CNN换成了先用卷积做prediction，再进行ROI pooling。由于ROI pooling会丢失位置信息，故在pooling前加入位置信息，即指定不同score map负责检测目标的不同位置。pooling后把不同位置得到的score map进行组合就能复现原来的位置信息。 Introduction分类需要特征具有平移不变性，检测则要求对目标的平移做出准确响应。现在的大部分CNN在分类上可以做的很好，但用在检测上效果不佳。SPP，Faster R-CNN类的方法在ROI pooling前都是卷积，是具备平移不变性的，但一旦插入ROI pooling之后，后面的网络结构就不再具备平移不变性了。因此，本文想提出来的position sensitive score map这个概念是能把目标的位置信息融合进ROI pooling。 对于region-based的检测方法，以Faster R-CNN为例，实际上是分成了几个subnetwork，第一个用来在整张图上做比较耗时的conv，这些操作与region无关，是计算共享的。第二个subnetwork是用来产生候选的boundingbox（如RPN），第三个subnetwork用来分类或进一步对box进行regression（如Fast RCNN），这个subnetwork和region是有关系的，必须每个region单独跑网络，衔接在这个subnetwork和前两个subnetwork中间的就是ROI pooling。我们希望的是，耗时的卷积都尽量移到前面共享的subnetwork上。因此，和Faster RCNN中用的ResNet（前91层共享，插入ROI pooling，后10层不共享）策略不同，本文把所有的101层都放在了前面共享的subnetwork。最后用来prediction的卷积只有1层，大大减少了计算量。 R-FCN Backbone architecture: ResNet 101——去掉原始ResNet101的最后一层全连接层，保留前100层，再接一个1*1*1024的全卷积层（100层输出是2048，为了降维，引入了一个1*1的卷积层）。 $k^2(C+1)$的conv: ResNet 101的输出是W*H*1024，用$k^2(C+1)$个1*1*1024的卷积核去卷积即可得到$k^2(C+1)$个大小为W*H的position sensitive的score map。这步的卷积操作就是在做prediction。例如k = 3，表示把一个ROI划分成3*3，对应的9个位置分别是：上左（左上角），上中，上右，中左，中中，中右，下左，下中，下右（右下角）。 $k^2(C+1)$个feature map的物理意义: 共有k*k = 9个颜色，每个颜色的立体块(W*H*(C+1))表示的是不同位置存在目标的概率值（第一块黄色表示的是左上角位置，最后一块淡蓝色表示的是右下角位置）。共有$k^2*(C+1)$个feature map。每个feature map，z(i,j,c)是第i+k(j-1)个立体块上的第c个map（1&lt;= i,j &lt;=3）。(i,j)决定了9种位置的某一种位置，假设为左上角位置（i=j=1），c决定了哪一类，假设为person类。在z(i,j,c)这个feature map上的某一个像素的位置是（x,y），像素值是value，则value表示的是原图对应的(x,y)这个位置上可能是人（c=‘person’）且是人的左上部位（i=j=1）的概率值。 ROI pooling的输入和输出：ROI pooling操作的输入（对于C+1个类）是$k^2\times (C+1)\times W’\times H’$（W’和H’是ROI的宽度和高度）的score map上某ROI对应的那个立体块，且该立体块组成一个新的$k^2\times (C+1)\times W’\times H’$的立体块：每个颜色的立体块（C+1）都只抠出对应位置的一个bin，把这k*k个bin组成新的立体块，大小为$(C+1)\times W’\times H’$。例如，上图中的第一块黄色只取左上角的bin，最后一块淡蓝色只取右下角的bin。所有的bin重新组合后就变成了类似右图的那个薄的立体块（图中的这个是池化后的输出，即每个面上的每个bin上已经是一个像素。池化前这个bin对应的是一个区域，是多个像素）。ROI pooling的输出为为一个$(C+1)\times k\times k$的立体块。 vote投票：k*k个bin直接进行求和（每个类单独做）得到每一类的score，并进行softmax得到每类的最终得分，并用于计算损失。 训练的样本选择策略：online hard example mining (OHEM) 。主要思想就是对样本按loss进行排序，选择前面loss较小的，这个策略主要用来对负样本进行筛选，使得正负样本更加平衡。 两个关键层： 包含多个 Score Map 的卷积层：把目标分割成了 k*k 个部分（比如33），每个部分映射到一张 Score Map 上，每个 Score Map 对应目标的一部分（如上图中的 top-left 左上角的 1/9）。最终得到 `kk` 个Score Map，每一个 Map通道数为 分类个数 C+1。 一个 ROI Pooling 层：这个 ROI 层仅针对上面的其中一个 Score Map 执行 Pooling 操作，重新排列成 k*k，通道数为 C+1。ROI Pooling 层通过 k*k 个 Part 进行投票，得到分类结果。 Reference R-FCN: Object Detection via Region-based Fully Convolutional Networks Caffe源码实现，Python版本 Caffe源码实现，Matlab版本 R-FCN算法及Caffe代码详解 RFCN论文笔记]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SSD]]></title>
    <url>%2F2018%2F04%2F21%2FSSD%2F</url>
    <content type="text"><![CDATA[这篇文章在既保证速度，又要保证精度的情况下，提出了SSD物体检测模型。SSD将检测过程整个成一个single deep neural network，便于训练与优化，同时提高检测速度。SSD将输出一系列离散化的bounding boxes，这些bounding boxes是在不同层次上的 feature maps上生成的，并且有着不同的aspect ratio。 Introduction SSD的特殊之处主要体现在以下3点： 多尺度的特征图检测（Multi-scale），如SSD同时使用了上图所示的8*8的特征图和4*4特征图。 相比于YOLO，作者使用的是卷积层来代替了YOLO的全连接层做预测。（如下图所示） SSD使用了默认的边界框+（1，2/1，3/1，1/2，1/3）6个框来做检测（aspect ratios） SSD网络训练技巧SSD能达到这么高的检测精度离不开它所采用的训练技巧。 1. 数据增强SSD训练过程中使用的数据增强对网络性能影响很大，大约有6.7%的mAP提升。 随机剪裁：采样一个片段，使剪裁部分与目标重叠分别为0.1, 0.3, 0.5, 0.7, 0.9，剪裁完resize到固定尺寸。 以0.5的概率随机水平翻转。 2. 是否在基础网络部分的conv4_3进行检测基础网络部分特征图分辨率高，原图中信息更完整，感受野较小，可以用来检测图像中的小目标，这也是SSD相对于YOLO检测小目标的优势所在。增加对基础网络conv4_3的特征图的检测可以使mAP提升4%。 3. 使用瘦高与宽扁默认框数据集中目标的开关往往各式各样，因此挑选合适形状的默认框能够提高检测效果。作者实验得出使用瘦高与宽扁默认框相对于只使用正方形默认框有2.9%mAP提升。 4. 使用atrous卷积通常卷积过程中为了使特征图尺寸特征图尺寸保持不变，通过会在边缘打padding，但人为加入的padding值会引入噪声，因此，使用atrous卷积能够在保持感受野不变的条件下，减少padding噪声，关于atrous参考。本文SSD训练过程中并且没有使用atrous卷积，但预训练过程使用的模型为VGG-16-atrous，意味着作者给的预训练模型是使用atrous卷积训练出来的。使用atrous版本VGG-16作为预训模型比较普通VGG-16要提高0.7%mAP。 Reference SSD: Single Shot MultiBox Detector (ECCV, 2016) [目标检测]SSD原理]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[YOLO]]></title>
    <url>%2F2018%2F04%2F21%2FYOLO%2F</url>
    <content type="text"><![CDATA[YOLO将物体检测作为一个回归问题进行求解，输入图像经过一次inference，便能得到图像中所有物体的位置和其所属类别及相应的置信概率。而rcnn/fast rcnn/faster rcnn将检测结果分为两部分求解：物体类别（分类问题），物体位置即bounding box（回归问题）。 IntroductionYOLO之前的物体检测系统使用分类器来完成物体检测任务。为了检测一个物体，这些物体检测系统要在一张测试图的不同位置和不同尺寸的bounding box上使用该物体的分类器去评估是否有该物体。如DPM系统，要使用一个滑窗（sliding window）在整张图像上均匀滑动，用分类器评估是否有物体。 在DPM之后提出的其他方法，如R-CNN方法使用region proposal来生成整张图像中可能包含待检测物体的potential bounding boxes，然后用分类器来评估这些boxes，接着通过post-processing来改善bounding boxes，消除重复的检测目标，并基于整个场景中的其他物体重新对boxes进行打分。整个流程执行下来很慢，而且因为这些环节都是分开训练的，检测性能很难进行优化。 作者设计了YOLO（you only look once），将物体检测任务当做回归问题（regression problem）来处理，直接通过整张图片的所有像素得到bounding box的坐标、box中包含物体的置信度和class probabilities。通过YOLO，每张图像只需要看一眼就能得出图像中都有哪些物体和这些物体的位置。 如图所示，使用YOLO来检测物体，其流程是非常简单明了的：1、将图像resize到448 * 448作为神经网络的输入；2、运行神经网络，得到一些bounding box坐标、box中包含物体的置信度和class probabilities；3、进行非极大值抑制，筛选Boxes。 ArchitectureYOLO检测网络包括24个卷积层和2个全连接层，如下图所示。 其中，卷积层用来提取图像特征，全连接层用来预测图像位置和类别概率值。YOLO网络借鉴了GoogLeNet分类网络结构。不同的是，YOLO未使用inception module，而是使用1x1卷积层（此处1x1卷积层的存在是为了跨通道信息整合）+3x3卷积层简单替代。 YOLOYOLO将输入图像划分为S*S的栅格，每个栅格负责检测中心落在该栅格中的物体,每一个栅格预测B个bounding boxes，以及这些bounding boxes的confidence scores。这个 confidence scores反映了模型对于这个栅格的预测：该栅格是否含有物体，以及这个box的坐标预测的有多准。公式定义如下： confidence = Pr(Object) * IOU_{pred}^{truth}如果这个栅格中不存在一个 object，则confidence score应该为0；否则的话，confidence score则为 predicted bounding box与 ground truth box之间的 IOU（intersection over union）。 YOLO对每个bounding box有5个predictions：x, y, w, h, and confidence。 坐标x,y代表了预测的bounding box的中心与栅格边界的相对值。 坐标w,h代表了预测的bounding box的width、height相对于整幅图像width,height的比例。 confidence就是预测的bounding box和ground truth box的IOU值。 每一个栅格还要预测C个 conditional class probability（条件类别概率）：Pr(Classi|Object)。即在一个栅格包含一个Object的前提下，它属于某个类的概率。我们只为每个栅格预测一组（C个）类概率，而不考虑框B的数量。 注意：conditional class probability信息是针对每个网格的。confidence信息是针对每个bounding box的。 在测试阶段，将每个栅格的conditional class probabilities与每个 bounding box的 confidence相乘： Pr(Class_i|Object) * Pr(Object) * IOU_{pred}^{truth} = Pr(Class_i) * IOU_{pred}^{truth}这样就得到了每个bounding box的具体类别的confidence score。这乘积既包含了bounding box中预测的class的 probability信息，也反映了bounding box是否含有Object和bounding box坐标的准确度。 将YOLO用于PASCAL VOC数据集时：论文使用的 S=7，即将一张图像分为7×7=49个栅格每一个栅格预测B=2个boxes（每个box有 x,y,w,h,confidence，5个预测值），同时C=20（PASCAL数据集中有20个类别）。因此，最后的prediction是7×7×30 { 即S S ( B * 5 + C) }的Tensor。 Loss损失函数的设计目标就是让坐标（x,y,w,h），confidence，classification 这个三个方面达到很好的平衡。简单的全部采用了sum-squared error loss来做这件事会有以下不足： 8维的localization error和20维的classification error同等重要显然是不合理的。 如果一些栅格中没有object（一幅图中这种栅格很多），那么就会将这些栅格中的bounding box的confidence 置为0，相比于较少的有object的栅格，这些不包含物体的栅格对梯度更新的贡献会远大于包含物体的栅格对梯度更新的贡献，这会导致网络不稳定甚至发散。 解决方案如下： 更重视8维的坐标预测，给这些损失前面赋予更大的loss weight, 记为 λcoord ,在pascal VOC训练中取5。（上图蓝色框） 对没有object的bbox的confidence loss，赋予小的loss weight，记为 λnoobj ，在pascal VOC训练中取0.5。（上图橙色框） 有object的bbox的confidence loss (上图红色框) 和类别的loss （上图紫色框）的loss weight正常取1。 对不同大小的bbox预测中，相比于大bbox预测偏一点，小box预测偏相同的尺寸对IOU的影响更大。而sum-square error loss中对同样的偏移loss是一样。为了缓和这个问题，作者用了一个巧妙的办法，就是将box的width和height取平方根代替原本的height和width。 如下图：small bbox的横轴值较小，发生偏移时，反应到y轴上的loss（下图绿色）比big box(下图红色)要大。 在 YOLO中，每个栅格预测多个bounding box，但在网络模型的训练中，希望每一个物体最后由一个bounding box predictor来负责预测。因此，当前哪一个predictor预测的bounding box与ground truth box的IOU最大，这个 predictor就负责 predict object。这会使得每个predictor可以专门的负责特定的物体检测。随着训练的进行，每一个 predictor对特定的物体尺寸、长宽比的物体的类别的预测会越来越好。 TrainingYOLO模型训练分为两步： 预训练。使用ImageNet 1000类数据训练YOLO网络的前20个卷积层+1个average池化层+1个全连接层。训练图像分辨率resize到224x224。 用步骤1得到的前20个卷积层网络参数来初始化YOLO模型前20个卷积层的网络参数，然后用VOC 20类标注数据进行YOLO模型训练。为提高图像精度，在训练检测模型时，将输入图像分辨率resize到448x448。 Inference 虽然每个格子可以预测B个bounding box，但是最终只选择只选择IOU最高的bounding box作为物体检测输出，即每个格子最多只预测出一个物体。当物体占画面比例较小，如图像中包含畜群或鸟群时，每个格子包含多个物体，但却只能检测出其中一个。这是YOLO方法的一个缺陷。 优缺点优点： 快。YOLO将物体检测作为回归问题进行求解，整个检测网络pipeline简单。在titan x GPU上，在保证检测准确率的前提下（63.4% mAP，VOC 2007 test set），可以达到45fps的检测速度。 背景误检率低。YOLO在训练和推理过程中能‘看到’整张图像的整体信息，而基于region proposal的物体检测方法（如rcnn/fast rcnn），在检测过程中，只‘看到’候选框内的局部图像信息。因此，若当图像背景（非物体）中的部分数据被包含在候选框中送入检测网络进行检测时，容易被误检测成物体。测试证明，YOLO对于背景图像的误检率低于fast rcnn误检率的一半。 通用性强。YOLO对于艺术类作品中的物体检测同样适用。它对非自然图像物体的检测率远远高于DPM和RCNN系列检测方法。 缺点： 识别物体位置精准性差。 召回率低。 Reference You Only Look Once: Unified, Real-Time Object Detection (CVPR, 2016) YOLO详解 YOLOv1论文理解]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客导航]]></title>
    <url>%2F2018%2F04%2F21%2F%E5%8D%9A%E5%AE%A2%E5%AF%BC%E8%88%AA%2F</url>
    <content type="text"><![CDATA[为方便浏览本博客的信息，建立了博客导航。 卷积神经网络结构相关论文笔记：Spatial Transformer Networks 目标检测相关论文笔记：R-CNN论文笔记：SPP-net论文笔记：Fast R-CNN论文笔记：Faster R-CNN论文笔记：YOLO论文笔记：SSD论文笔记：R-FCN]]></content>
  </entry>
  <entry>
    <title><![CDATA[Faster R-CNN]]></title>
    <url>%2F2018%2F04%2F15%2FFaster-R-CNN%2F</url>
    <content type="text"><![CDATA[Faster R-CCN已经将特征提取(feature extraction)，proposal提取，bounding box regression，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。 Introduction如下图所示，Faster R-CNN其实可以分为4个主要内容： Conv layers。作为一种CNN网络目标检测方法，Faster R-CNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。 Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。 Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。 Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。 Faster R-CNN下图展示了Python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构，可以清晰的看到该网络对于一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络；而Conv layers中包含了13个conv层+13个relu层+4个pooling层；RPN网络首先经过3x3卷积，再分别生成foreground anchors与bounding box regression偏移量，然后计算出proposals；而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。 Conv layersConv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中： 所有的conv层都是：kernel_size=3，pad=1 所有的pooling层都是：kernel_size=2，stride=2 为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（pad=1，即填充一圈0），导致原图变为(M+2)x(N+2)大小，再做3x3卷积后输出MxN。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如下图： 类似的是，Conv layers中的pooling层kernel_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)*(N/2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。 那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！这样Conv layers生成的featuure map中都可以和原图对应起来。 Region Proposal Networks (RPN)经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如RCNN使用SS(Selective Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster RCNN的巨大优势，能极大提升检测框的生成速度。 上图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。 RoI poolingRoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图3中可以看到Rol pooling层有2个输入： 原始的feature maps RPN输出的proposal boxes（大小各不相同） ClassificationClassification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如下图： 从PoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事： 通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了 再次对proposals进行bounding box regression，获取更高精度的rect box Reference Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks Caffe源码实现，Python版本 Caffe源码实现，Matlab版本 faster-RCNN算法原理详解]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast R-CNN]]></title>
    <url>%2F2018%2F04%2F15%2FFast-R-CNN%2F</url>
    <content type="text"><![CDATA[Fast R-CNN是对R-CNN算法的改进，采用了一些创新措施来提高训练和测试速度，同时提高检测准确率。Fast R-CNN训练VGG16网络比R-CNN快9倍，测试速度快213倍，并在PASCAL VOC上得到更高的精度。与SPPnet相比，Fast R-CNN训练VGG16网络比它快3倍，测试速度快10倍，并且更准确。 IntroductionR-CNN有以下缺点： 训练分为多个步骤：R-CNN首先需要微调CNN，然后使用SVM对CNN提取的特征进行分类，最后还要训练一个bounding-box的回归器。 训练过程的空间和时间代价高：对于SVM和bounding-box回归器的训练，需要保存每幅图像的每个proposal的特征。 目标检测速度慢：采用VGG16测试一张图片需要47s（在GPU上）。 R-CNN检测速度慢的原因在于使用CNN对每个proposal进行前向运算时没有共享计算。SPPnet采用了共享计算来加速R-CNN，即对输入图像只进行一次前向运算，然后从最后共享的feature map中提取出每个proposal的特征向量进行分类。SPPnet在测试阶段比R-CNN快了10-100倍。由于更快的特征提取，所以训练时间也减少了近3倍。SPPnet也有明显的缺点，和R-CNN一样，训练过程也分为多个步骤，也需要保存中间的特征。 Fast R-CNN下图是Fast R-CNN的框架，步骤如下： 第一步，将这个完整的图片经过若干卷积层与max pooling层，得到一个feature map。 第二步，用selective search算法从这完整的图片中提取出object proposals，即RoI。 第三步，根据映射关系，可以得到每个object proposal对应的feature map。 第四步，将第三步得到的feature map经过RoI pooling layer得到固定大小的feature map（变小了）。 第五步，经过2层全连接层（fc），得到固定大小的RoI特征向量。 第六步，特征向量经由各自的FC层，得到两个输出向量：第一个是分类，使用softmax，第二个是每一类的bounding box回归。 简要流程图如下： Reference Fast R-CNN Caffe源码实现 Fast R-CNN笔记 【目标检测】Fast RCNN算法详解]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SPP-net]]></title>
    <url>%2F2018%2F04%2F14%2FSPP-net%2F</url>
    <content type="text"><![CDATA[为了解决现有CNN需要固定输入大小的问题，提出了SPP-net，使得针对任意尺寸的图像生成固定长度的特征表示。SPP-net不仅可以应用在分类任务上，而且在检测任务上也有很大的性能提升。 IntroductionCNN正在取得快速的发展，然而在CNN的训练和测试阶段都有一个技术问题：CNN需要固定输入图像的尺寸，这些图片或者经过裁切（crop）或者经过变形缩放（warp），都在一定程度上导致图片信息的丢失和变形，限制了识别精确度。 如下图所示，上面是CNN一般的做法，对不符合网络输入大小的图像直接进行crop或warp，下面是SPP-net的工作方式。SPP-net加在最后一个卷积层的输出后面，使得不同输入尺寸的图像在进过前面的卷积池化过程后，再经过SPP-net，得到相同大小的feature map，最后再经过全连接层进行分类。 Spatital Pyramid Pooling卷积层是不需要输入固定大小的图片的，而且还可以生成任意大小的特征图，只是全连接层需要固定大小的输入。因此，固定长度的约束仅限于全连接层。在本文中提出了Spatial Pyramid Pooling layer 来解决这一问题，使用这种方式，可以让网络输入任意的图片，而且还会生成固定大小的输出。 以下图为例，黑色图片代表卷积之后的特征图，接着我们以不同大小的块来提取特征，分别是4*4，2*2，1*1，将这三张网格放到下面这张特征图上，就可以得到16+4+1=21种不同的块(Spatial bins)，我们从这21个块中，每个块提取出一个特征，这样刚好就是我们要提取的21维特征向量。这种以不同的大小格子的组合方式来池化的过程就是空间金字塔池化（SPP）。比如，要进行空间金字塔最大池化，其实就是从这21个图片块中，分别计算每个块的最大值，从而得到一个输出单元，最终得到一个21维特征的输出。 金字塔池化的意义总结而言，当网络输入的是一张任意大小的图片，这个时候我们可以一直进行卷积、池化，直到网络的倒数几层的时候，也就是我们即将与全连接层连接的时候，就要使用金字塔池化，使得任意大小的特征图都能够转换成固定大小的特征向量，这就是空间金字塔池化的意义（多尺度特征提取出固定大小的特征向量）。 SPP-net在目标检测中的应用对于R-CNN，整个过程是： 首先通过选择性搜索，对待检测的图片进行搜索出~2000个候选窗口。 把这2k个候选窗口的图片都缩放到227*227，然后分别输入CNN中，每个proposal提取出一个特征向量，也就是说利用CNN对每个proposal进行提取特征向量。 把上面每个候选窗口的对应特征向量，利用SVM算法进行分类识别。 可以看出R-CNN的计算量是非常大的，因为2k个候选窗口都要输入到CNN中，分别进行特征提取。 而对于SPP-Net，整个过程是： 首先通过选择性搜索，对待检测的图片进行搜索出2000个候选窗口。这一步和R-CNN一样。 特征提取阶段。这一步就是和R-CNN最大的区别了，这一步骤的具体操作如下：把整张待检测的图片，输入CNN中，进行一次性特征提取，得到feature maps，然后在feature maps中找到各个候选框的区域，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量。而R-CNN输入的是每个候选框，然后在进入CNN，因为SPP-Net只需要一次对整张图片进行特征提取，速度会大大提升。 最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别。 下图为SPP-net进行目标检测的完整步骤： Reference Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (PAMI, 2015) SPPnet论文总结 SPP-Net论文详解 Caffe源码实现]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-CNN]]></title>
    <url>%2F2018%2F04%2F14%2FR-CNN%2F</url>
    <content type="text"><![CDATA[R-CNN，结合Region Proposal和CNN的目标检测算法。使用Region Proposal (Selective Search)算法得到有可能是目标的若干图像局部区域，然后把这些区域分别输入到CNN中，得到区域的特征，加上SVM分类器，判断特征对应的区域是属于某类目标还是背景，最后进行边框回归。 Introduction在R-CNN之前，目标检测已经到了瓶颈期，一些最好的方法都是结合底层图像特征和高层语义的复杂集成系统。而这篇论文在VOC2012的mAP达到了53.3%，比之前最好的方法高30%。该方法主要有两个关键点：1）一个是将CNN应用在了自底向上的区域候选框上，用来定位和分割目标；2）当标记的训练数据比较少时，采用有监督预训练，并进行微调，可以显著提高性能。 R-CNNOverviewR-CNN的目标检测系统由三部分组成：第一步生成region proposal，采用selective search算法；第二步使用CNN提取固定长度的特征向量；第三步使用线性SVM分类。 _^. &#8617; Training 有监督预训练：在ILSVRC2012分类数据集上预训练。 微调：使用目标检测的数据进行微调，以0.001的学习速率进行SGD训练。对某个分类只要IoU&gt;0.5就视该边框为正值。每次SGD迭代都采样38个正边框和96个背景。 分类器：对某个分类，高IoU和低IoU都很好区分，但IoU处于中值时则很难定义生成的候选框是否包含了该物体。作者设定了一个阈值0.3（从一系列阈值中选择的，0,0.1,…,0.5），低于它的一律视为背景（负数）。另外，每个分类都优化一个SVM。由于负样本很多，因此还采用了hard negative mining方法。 Testing 采用selective search提取约2000个region proposal，将每个region proposal缩放成227*227，利用CNN前向运算计算特征。然后，使用训练好的SVM对特征进行分类，最后采用NMS去除多余的region。 运行时间分析：有两个因素使得检测效率较高，首先所有CNN参数对所有类别都是共享的，其次，由CNN计算出来的特征向量相比其他方法是低维特征。 一些问题 可以不进行特定样本下的微调吗？可以直接采用AlexNet网络的特征进行SVM训练吗? ⽂中设计了没有进⾏微调的对⽐实验，分别就AlexNet⽹络的pool5、fc6、fc7层进⾏特征提取，输⼊SVM进⾏训练，这相当于把AlexNet网络作为特征提取器，类似HOG、SIFT等做特征提取⼀样，不针对特征任务。实验结果发现f6层提取的特征⽐f7层的mAP还⾼，pool5层提取的特征与f6、f7层相⽐mAP差不多； 在PASCAL VOC 2007数据集上采取了微调后fc6、fc7层特征较pool5层特征⽤于SVM训练提升mAP⼗分明显； 由此作者得出结论：不针对特定任务进⾏微调，⽽将CNN当成特征提取器，pool5层得到的特征是基础特征，类似于HOG、SIFT，类似于只学习到了⼈脸共性特征；从fc6和fc7等全连接层中所学习到的特征是针对特征任务特定样本的特征，类似于学习到了分类性别分类年龄的个性特征。 为什么微调时和训练SVM时所采⽤的正负样本阈值【0.5和0.3】不⼀致？ 微调阶段是由于CNN对⼩样本容易过拟合，需要⼤量训练数据，故对IoU限制宽松：与Ground Truth相交IoU&gt;0.5的建议框为正样本，否则为负样本； SVM这种机制是由于其适⽤于⼩样本训练，故对样本IoU限制严格：Ground Truth为正样本，与Ground Truth相交IoU＜0.3的建议框为负样本。 为什么不直接采⽤微调后的AlexNet⽹络最后⼀层SoftMax进⾏21分类【20类+背景】？ 因为微调时和训练SVM时所采⽤的正负样本阈值不同，微调阶段正样本定义并不强调精准的位置，⽽SVM正样本只有Ground Truth；并且微调阶段的负样本是随机抽样的，⽽SVM的负样本是经过hard negative mining⽅法筛选的；导致在采⽤SoftMax会使PSACAL VOC 2007测试集上mAP从54.2%降低到50.9%。 R-CNN速度慢在哪里？ RCNN存在着重复计算的问题（proposal的region有几千个，多数都是互相重叠，重叠部分会被多次重复提取feature）。 Reference Rich feature hierarchies for accurate object detection and semantic segmentation Caffe源码实现 R-CNN论文详解]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spatial Transformer Networks]]></title>
    <url>%2F2018%2F04%2F12%2FSpatial-Transformer-Networks%2F</url>
    <content type="text"><![CDATA[传统CNN通过扩增数据获得数据的一些不变性，如旋转不变性，平移不变性等，是一种隐式的学习，而空间变换网络（Spatial Transformer Networks，简称STN）通过显式学习数据的各种变换参数（如旋转，平移，仿射变换等）来获得这些变换的不变性。这个网络可以很方便的插入到已有的CNN网络中。 Introduction 不变性（invariance）：在CNN中，pooling层使得网络对位置的敏感度越来越低，从而使网络具有一定的平移不变性。这种空间不变性是通过多层的conv和pooling层实现的，而且对变换较大的输入数据并不具有不变性。 Spatial Transformer: 引入空间变换模块（spatial transformer module），可以添加到任何一个标准的神经网络结构中，从而提供空间变换的能力。与pooling层不同，空间变换模块是一个动态的机制，训练得到合适的变换参数，然后将这个变换应用在整个feature map上，可以覆盖scaling, cropping, ratations等变换。这样的好处是可以使得网络选择出一幅图像中最相关的区域，如下图所示，(a)是输入图像，(b)是空间变换模块预测的定位结果，(c)是空间变换后的结果，(d)是分类结果。 应用：STN可以嵌入CNN中实现不同的任务，例如： image classification: STN可以crop out和scale-normalize合适的区域，简化接下来的分类任务，提高分类性能； co-localisation: 不需要标记目标的位置，通过STN，可以定位目标； spatial attention: 可以实现attention机制，很灵活，无需增强学习。 _^. &#8617; Spatial Transformers由以下三部分组成 _^. &#8617; Localisation Network: 输入feature map，输出$\theta$。$\theta$的尺寸取决于变换类型，比如对于仿射变换，$\theta$就是6维的。这个网络可以采用任意形式，比如全连接网络或卷积网络，但是最后应该有一个回归层输出参数$\theta$。 Parameterised Sampling Grid: 以下是将Parameterised Sampling Grid应用到图像U上得到输出V的两个例子，图(a)是一个单位变换，图(b)是一个仿射变换。 _^. &#8617; Differentiable Image Sampling: 这一步是将输入的feature map结合变换参数，输出结果feature map。 个人总结传统CNN通过各种数据扩增操作来获得某些不变性（如平移不变性，旋转不变性等），是一种“隐式学习”的方法，即我们只是向网络输入数据，直接输出的就是预测结果。而STN主要是一种“显式学习”空间变换参数的方法，它通过训练得到这些参数，对输入数据（也可以是中间的feature map）进行变换，使得后续的处理能得到更好的结果。 优点：容易扩展，可以很方便的嵌入其它网络中；与CNN是直接识别出扭曲图像不同，STN显然更符合人类识别的过程，即先将图像纠正再识别。 缺点：训练时依然需要大量数据扩增才能学到所需要的不变性，而且学习到的变换参数不一定是我们希望的。 复现过程：对Mnist数据集进行随机旋转，然后使用论文中提到的超参数设置，结果显示采用STN后在测试集上的准确率确实高于不使用STN的时候。但是对STN层后的输出进行可视化后并没有原文中效果那么好，比如我需要学习的是旋转不变性，但是结果显示STN层所做的事情是平移而不是旋转。我认为可能在学习旋转不变性时需要固定6个变换参数中的某几个，使得能够显式的学习旋转不变性。 Reference Spatial Transformer Networks (NIPS, 2015) 李宏毅-深度学习-Spatial Transformer Layer Paper Reading：Spatial Transformer Networks（with code explanation） Caffe实现版本 Tensorflow实现版本 PyTorch实现版本 MXNet实现版本]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
</search>
