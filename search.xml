<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Faster R-CNN]]></title>
    <url>%2F2018%2F04%2F15%2FFaster-R-CNN%2F</url>
    <content type="text"><![CDATA[Faster R-CCN已经将特征提取(feature extraction)，proposal提取，bounding box regression，classification都整合在了一个网络中，使得综合性能有较大提高，在检测速度方面尤为明显。 Introduction如下图所示，Faster R-CNN其实可以分为4个主要内容： Conv layers。作为一种CNN网络目标检测方法，Faster R-CNN首先使用一组基础的conv+relu+pooling层提取image的feature maps。该feature maps被共享用于后续RPN层和全连接层。 Region Proposal Networks。RPN网络用于生成region proposals。该层通过softmax判断anchors属于foreground或者background，再利用bounding box regression修正anchors获得精确的proposals。 Roi Pooling。该层收集输入的feature maps和proposals，综合这些信息后提取proposal feature maps，送入后续全连接层判定目标类别。 Classification。利用proposal feature maps计算proposal的类别，同时再次bounding box regression获得检测框最终的精确位置。 Faster R-CNN下图展示了Python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构，可以清晰的看到该网络对于一副任意大小PxQ的图像，首先缩放至固定大小MxN，然后将MxN图像送入网络；而Conv layers中包含了13个conv层+13个relu层+4个pooling层；RPN网络首先经过3x3卷积，再分别生成foreground anchors与bounding box regression偏移量，然后计算出proposals；而Roi Pooling层则利用proposals从feature maps中提取proposal feature送入后续全连接和softmax网络作classification（即分类proposal到底是什么object）。 Conv layersConv layers包含了conv，pooling，relu三种层。以python版本中的VGG16模型中的faster_rcnn_test.pt的网络结构为例，如图2，Conv layers部分共有13个conv层，13个relu层，4个pooling层。这里有一个非常容易被忽略但是又无比重要的信息，在Conv layers中： 所有的conv层都是：kernel_size=3，pad=1 所有的pooling层都是：kernel_size=2，stride=2 为何重要？在Faster RCNN Conv layers中对所有的卷积都做了扩边处理（pad=1，即填充一圈0），导致原图变为(M+2)x(N+2)大小，再做3x3卷积后输出MxN。正是这种设置，导致Conv layers中的conv层不改变输入和输出矩阵大小。如下图： 类似的是，Conv layers中的pooling层kernel_size=2，stride=2。这样每个经过pooling层的MxN矩阵，都会变为(M/2)*(N/2)大小。综上所述，在整个Conv layers中，conv和relu层不改变输入输出大小，只有pooling层使输出长宽都变为输入的1/2。 那么，一个MxN大小的矩阵经过Conv layers固定变为(M/16)x(N/16)！这样Conv layers生成的featuure map中都可以和原图对应起来。 Region Proposal Networks (RPN)经典的检测方法生成检测框都非常耗时，如OpenCV adaboost使用滑动窗口+图像金字塔生成检测框；或如RCNN使用SS(Selective Search)方法生成检测框。而Faster RCNN则抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster RCNN的巨大优势，能极大提升检测框的生成速度。 上图展示了RPN网络的具体结构。可以看到RPN网络实际分为2条线，上面一条通过softmax分类anchors获得foreground和background（检测目标是foreground），下面一条用于计算对于anchors的bounding box regression偏移量，以获得精确的proposal。而最后的Proposal层则负责综合foreground anchors和bounding box regression偏移量获取proposals，同时剔除太小和超出边界的proposals。其实整个网络到了Proposal Layer这里，就完成了相当于目标定位的功能。 RoI poolingRoI Pooling层则负责收集proposal，并计算出proposal feature maps，送入后续网络。从图3中可以看到Rol pooling层有2个输入： 原始的feature maps RPN输出的proposal boxes（大小各不相同） ClassificationClassification部分利用已经获得的proposal feature maps，通过full connect层与softmax计算每个proposal具体属于那个类别（如人，车，电视等），输出cls_prob概率向量；同时再次利用bounding box regression获得每个proposal的位置偏移量bbox_pred，用于回归更加精确的目标检测框。Classification部分网络结构如下图： 从PoI Pooling获取到7x7=49大小的proposal feature maps后，送入后续网络，可以看到做了如下2件事： 通过全连接和softmax对proposals进行分类，这实际上已经是识别的范畴了 再次对proposals进行bounding box regression，获取更高精度的rect box Reference Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks Caffe源码实现，Python版本 Caffe源码实现，Matlab版本 faster-RCNN算法原理详解]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fast R-CNN]]></title>
    <url>%2F2018%2F04%2F15%2FFast-R-CNN%2F</url>
    <content type="text"><![CDATA[Fast R-CNN是对R-CNN算法的改进，采用了一些创新措施来提高训练和测试速度，同时提高检测准确率。Fast R-CNN训练VGG16网络比R-CNN快9倍，测试速度快213倍，并在PASCAL VOC上得到更高的精度。与SPPnet相比，Fast R-CNN训练VGG16网络比它快3倍，测试速度快10倍，并且更准确。 IntroductionR-CNN有以下缺点： 训练分为多个步骤：R-CNN首先需要微调CNN，然后使用SVM对CNN提取的特征进行分类，最后还要训练一个bounding-box的回归器。 训练过程的空间和时间代价高：对于SVM和bounding-box回归器的训练，需要保存每幅图像的每个proposal的特征。 目标检测速度慢：采用VGG16测试一张图片需要47s（在GPU上）。 R-CNN检测速度慢的原因在于使用CNN对每个proposal进行前向运算时没有共享计算。SPPnet采用了共享计算来加速R-CNN，即对输入图像只进行一次前向运算，然后从最后共享的feature map中提取出每个proposal的特征向量进行分类。SPPnet在测试阶段比R-CNN快了10-100倍。由于更快的特征提取，所以训练时间也减少了近3倍。SPPnet也有明显的缺点，和R-CNN一样，训练过程也分为多个步骤，也需要保存中间的特征。 Fast R-CNN下图是Fast R-CNN的框架，步骤如下： 第一步，将这个完整的图片经过若干卷积层与max pooling层，得到一个feature map。 第二步，用selective search算法从这完整的图片中提取出object proposals，即RoI。 第三步，根据映射关系，可以得到每个object proposal对应的feature map。 第四步，将第三步得到的feature map经过RoI pooling layer得到固定大小的feature map（变小了）。 第五步，经过2层全连接层（fc），得到固定大小的RoI特征向量。 第六步，特征向量经由各自的FC层，得到两个输出向量：第一个是分类，使用softmax，第二个是每一类的bounding box回归。 简要流程图如下： Reference Fast R-CNN Caffe源码实现 Fast R-CNN笔记 【目标检测】Fast RCNN算法详解]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SPP-net]]></title>
    <url>%2F2018%2F04%2F14%2FSPP-net%2F</url>
    <content type="text"><![CDATA[为了解决现有CNN需要固定输入大小的问题，提出了SPP-net，使得针对任意尺寸的图像生成固定长度的特征表示。SPP-net不仅可以应用在分类任务上，而且在检测任务上也有很大的性能提升。 IntroductionCNN正在取得快速的发展，然而在CNN的训练和测试阶段都有一个技术问题：CNN需要固定输入图像的尺寸，这些图片或者经过裁切（crop）或者经过变形缩放（warp），都在一定程度上导致图片信息的丢失和变形，限制了识别精确度。 如下图所示，上面是CNN一般的做法，对不符合网络输入大小的图像直接进行crop或warp，下面是SPP-net的工作方式。SPP-net加在最后一个卷积层的输出后面，使得不同输入尺寸的图像在进过前面的卷积池化过程后，再经过SPP-net，得到相同大小的feature map，最后再经过全连接层进行分类。 Spatital Pyramid Pooling卷积层是不需要输入固定大小的图片的，而且还可以生成任意大小的特征图，只是全连接层需要固定大小的输入。因此，固定长度的约束仅限于全连接层。在本文中提出了Spatial Pyramid Pooling layer 来解决这一问题，使用这种方式，可以让网络输入任意的图片，而且还会生成固定大小的输出。 以下图为例，黑色图片代表卷积之后的特征图，接着我们以不同大小的块来提取特征，分别是4*4，2*2，1*1，将这三张网格放到下面这张特征图上，就可以得到16+4+1=21种不同的块(Spatial bins)，我们从这21个块中，每个块提取出一个特征，这样刚好就是我们要提取的21维特征向量。这种以不同的大小格子的组合方式来池化的过程就是空间金字塔池化（SPP）。比如，要进行空间金字塔最大池化，其实就是从这21个图片块中，分别计算每个块的最大值，从而得到一个输出单元，最终得到一个21维特征的输出。 金字塔池化的意义总结而言，当网络输入的是一张任意大小的图片，这个时候我们可以一直进行卷积、池化，直到网络的倒数几层的时候，也就是我们即将与全连接层连接的时候，就要使用金字塔池化，使得任意大小的特征图都能够转换成固定大小的特征向量，这就是空间金字塔池化的意义（多尺度特征提取出固定大小的特征向量）。 SPP-net在目标检测中的应用对于R-CNN，整个过程是： 首先通过选择性搜索，对待检测的图片进行搜索出~2000个候选窗口。 把这2k个候选窗口的图片都缩放到227*227，然后分别输入CNN中，每个proposal提取出一个特征向量，也就是说利用CNN对每个proposal进行提取特征向量。 把上面每个候选窗口的对应特征向量，利用SVM算法进行分类识别。 可以看出R-CNN的计算量是非常大的，因为2k个候选窗口都要输入到CNN中，分别进行特征提取。 而对于SPP-Net，整个过程是： 首先通过选择性搜索，对待检测的图片进行搜索出2000个候选窗口。这一步和R-CNN一样。 特征提取阶段。这一步就是和R-CNN最大的区别了，这一步骤的具体操作如下：把整张待检测的图片，输入CNN中，进行一次性特征提取，得到feature maps，然后在feature maps中找到各个候选框的区域，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量。而R-CNN输入的是每个候选框，然后在进入CNN，因为SPP-Net只需要一次对整张图片进行特征提取，速度会大大提升。 最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别。 下图为SPP-net进行目标检测的完整步骤： Reference Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition (PAMI, 2015) SPPnet论文总结 SPP-Net论文详解 Caffe源码实现]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R-CNN]]></title>
    <url>%2F2018%2F04%2F14%2FR-CNN%2F</url>
    <content type="text"><![CDATA[R-CNN，结合Region Proposal和CNN的目标检测算法。使用Region Proposal (Selective Search)算法得到有可能是目标的若干图像局部区域，然后把这些区域分别输入到CNN中，得到区域的特征，加上SVM分类器，判断特征对应的区域是属于某类目标还是背景，最后进行边框回归。 Introduction在R-CNN之前，目标检测已经到了瓶颈期，一些最好的方法都是结合底层图像特征和高层语义的复杂集成系统。而这篇论文在VOC2012的mAP达到了53.3%，比之前最好的方法高30%。该方法主要有两个关键点：1）一个是将CNN应用在了自底向上的区域候选框上，用来定位和分割目标；2）当标记的训练数据比较少时，采用有监督预训练，并进行微调，可以显著提高性能。 R-CNNOverviewR-CNN的目标检测系统由三部分组成：第一步生成region proposal，采用selective search算法；第二步使用CNN提取固定长度的特征向量；第三步使用线性SVM分类。 Training 有监督预训练：在ILSVRC2012分类数据集上预训练。 微调：使用目标检测的数据进行微调，以0.001的学习速率进行SGD训练。对某个分类只要IoU&gt;0.5就视该边框为正值。每次SGD迭代都采样38个正边框和96个背景。 分类器：对某个分类，高IoU和低IoU都很好区分，但IoU处于中值时则很难定义生成的候选框是否包含了该物体。作者设定了一个阈值0.3（从一系列阈值中选择的，0,0.1,…,0.5），低于它的一律视为背景（负数）。另外，每个分类都优化一个SVM。由于负样本很多，因此还采用了hard negative mining方法。 Testing 采用selective search提取约2000个region proposal，将每个region proposal缩放成227*227，利用CNN前向运算计算特征。然后，使用训练好的SVM对特征进行分类，最后采用NMS去除多余的region。 运行时间分析：有两个因素使得检测效率较高，首先所有CNN参数对所有类别都是共享的，其次，由CNN计算出来的特征向量相比其他方法是低维特征。 一些问题 可以不进行特定样本下的微调吗？可以直接采用AlexNet网络的特征进行SVM训练吗? ⽂中设计了没有进⾏微调的对⽐实验，分别就AlexNet⽹络的pool5、fc6、fc7层进⾏特征提取，输⼊SVM进⾏训练，这相当于把AlexNet网络作为特征提取器，类似HOG、SIFT等做特征提取⼀样，不针对特征任务。实验结果发现f6层提取的特征⽐f7层的mAP还⾼，pool5层提取的特征与f6、f7层相⽐mAP差不多； 在PASCAL VOC 2007数据集上采取了微调后fc6、fc7层特征较pool5层特征⽤于SVM训练提升mAP⼗分明显； 由此作者得出结论：不针对特定任务进⾏微调，⽽将CNN当成特征提取器，pool5层得到的特征是基础特征，类似于HOG、SIFT，类似于只学习到了⼈脸共性特征；从fc6和fc7等全连接层中所学习到的特征是针对特征任务特定样本的特征，类似于学习到了分类性别分类年龄的个性特征。 为什么微调时和训练SVM时所采⽤的正负样本阈值【0.5和0.3】不⼀致？ 微调阶段是由于CNN对⼩样本容易过拟合，需要⼤量训练数据，故对IoU限制宽松：与Ground Truth相交IoU&gt;0.5的建议框为正样本，否则为负样本； SVM这种机制是由于其适⽤于⼩样本训练，故对样本IoU限制严格：Ground Truth为正样本，与Ground Truth相交IoU＜0.3的建议框为负样本。 为什么不直接采⽤微调后的AlexNet⽹络最后⼀层SoftMax进⾏21分类【20类+背景】？ 因为微调时和训练SVM时所采⽤的正负样本阈值不同，微调阶段正样本定义并不强调精准的位置，⽽SVM正样本只有Ground Truth；并且微调阶段的负样本是随机抽样的，⽽SVM的负样本是经过hard negative mining⽅法筛选的；导致在采⽤SoftMax会使PSACAL VOC 2007测试集上mAP从54.2%降低到50.9%。 R-CNN速度慢在哪里？ RCNN存在着重复计算的问题（proposal的region有几千个，多数都是互相重叠，重叠部分会被多次重复提取feature）。 Reference Rich feature hierarchies for accurate object detection and semantic segmentation Caffe源码实现 R-CNN论文详解]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
        <tag>Object detection</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spatial Transformer Networks]]></title>
    <url>%2F2018%2F04%2F12%2FSpatial-Transformer-Networks%2F</url>
    <content type="text"><![CDATA[传统CNN通过扩增数据获得数据的一些不变性，如旋转不变性，平移不变性等，是一种隐式的学习，而空间变换网络（Spatial Transformer Networks，简称STN）通过显式学习数据的各种变换参数（如旋转，平移，仿射变换等）来获得这些变换的不变性。这个网络可以很方便的插入到已有的CNN网络中。 Introduction 不变性（invariance）：在CNN中，pooling层使得网络对位置的敏感度越来越低，从而使网络具有一定的平移不变性。这种空间不变性是通过多层的conv和pooling层实现的，而且对变换较大的输入数据并不具有不变性。 Spatial Transformer: 引入空间变换模块（spatial transformer module），可以添加到任何一个标准的神经网络结构中，从而提供空间变换的能力。与pooling层不同，空间变换模块是一个动态的机制，训练得到合适的变换参数，然后将这个变换应用在整个feature map上，可以覆盖scaling, cropping, ratations等变换。这样的好处是可以使得网络选择出一幅图像中最相关的区域，如下图所示，(a)是输入图像，(b)是空间变换模块预测的定位结果，(c)是空间变换后的结果，(d)是分类结果。 应用：STN可以嵌入CNN中实现不同的任务，例如： image classification: STN可以crop out和scale-normalize合适的区域，简化接下来的分类任务，提高分类性能； co-localisation: 不需要标记目标的位置，通过STN，可以定位目标； spatial attention: 可以实现attention机制，很灵活，无需增强学习。 Spatial Transformers由以下三部分组成 Localisation Network: 输入feature map，输出$\theta$。$\theta$的尺寸取决于变换类型，比如对于仿射变换，$\theta$就是6维的。这个网络可以采用任意形式，比如全连接网络或卷积网络，但是最后应该有一个回归层输出参数$\theta$。 Parameterised Sampling Grid: 以下是将Parameterised Sampling Grid应用到图像U上得到输出V的两个例子，图(a)是一个单位变换，图(b)是一个仿射变换。 Differentiable Image Sampling: 这一步是将输入的feature map结合变换参数，输出结果feature map。 个人总结传统CNN通过各种数据扩增操作来获得某些不变性（如平移不变性，旋转不变性等），是一种“隐式学习”的方法，即我们只是向网络输入数据，直接输出的就是预测结果。而STN主要是一种“显式学习”空间变换参数的方法，它通过训练得到这些参数，对输入数据（也可以是中间的feature map）进行变换，使得后续的处理能得到更好的结果。 优点：容易扩展，可以很方便的嵌入其它网络中；与CNN是直接识别出扭曲图像不同，STN显然更符合人类识别的过程，即先将图像纠正再识别。 缺点：训练时依然需要大量数据扩增才能学到所需要的不变性，而且学习到的变换参数不一定是我们希望的。 复现过程：对Mnist数据集进行随机旋转，然后使用论文中提到的超参数设置，结果显示采用STN后在测试集上的准确率确实高于不使用STN的时候。但是对STN层后的输出进行可视化后并没有原文中效果那么好，比如我需要学习的是旋转不变性，但是结果显示STN层所做的事情是平移而不是旋转。我认为可能在学习旋转不变性时需要固定6个变换参数中的某几个，使得能够显式的学习旋转不变性。 Reference Spatial Transformer Networks (NIPS, 2015) 李宏毅-深度学习-Spatial Transformer Layer Paper Reading：Spatial Transformer Networks（with code explanation） Caffe实现版本 Tensorflow实现版本 PyTorch实现版本 MXNet实现版本]]></content>
      <categories>
        <category>论文笔记</category>
      </categories>
      <tags>
        <tag>Deep learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
</search>
